{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc6e0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a function to oobfuscate the store names and introduce impurities for the sake of this excercise\n",
    "# Scrapped or OCR results could be much more random and unpredictable.\n",
    "import string\n",
    "import random \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "seen = {}\n",
    "symbols = \"!@#$%^&*()-+,<.>?/'' \"\n",
    "numbers = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "\n",
    "def gen_impurities(n,word):\n",
    "    for i in range(n):\n",
    "        index = random.randint(0,len(word)-1)\n",
    "        #print(f\"impute index - {index}\")\n",
    "\n",
    "        if index%2 == 0:\n",
    "            symbol = random.choice(symbols)\n",
    "            #print(f\"Symbol = {symbol}\")\n",
    "            word = word[:index] + symbol + word[index:]\n",
    "\n",
    "        if index%7 == 0:\n",
    "            num = random.choice(numbers)\n",
    "            #print(f\"Number = {num}\")\n",
    "            word = word[:index] + str(num) + word[index:]\n",
    "\n",
    "        if index%3 == 0:\n",
    "            word.replace(word[index],\"\")\n",
    "    return word\n",
    "\n",
    "def obfuscate(word):\n",
    "    n = int(len(word)*0.5)\n",
    "#     print(n)\n",
    "    iteration = random.randint(1,n)\n",
    "#     print(iteration)\n",
    "    label = word\n",
    "    for i in range(5):\n",
    "        imputed_word = gen_impurities(iteration,label)\n",
    "#         print(imputed_word)\n",
    "        if imputed_word not in seen:\n",
    "            seen[imputed_word] = 1\n",
    "            return imputed_word\n",
    "        seen[imputed_word] = seen[imputed_word]+1\n",
    "    return word\n",
    "            \n",
    "\n",
    "def gen_gibberish(min_l,max_l):\n",
    "    # initializing size of string\n",
    "    l = random.randint(min_l,max_l)\n",
    "    # using random.choices()\n",
    "    # generating random strings\n",
    "    res = ''.join(random.choices(string.ascii_uppercase +\n",
    "                                 string.punctuation +\n",
    "                                 string.digits, k=l))\n",
    "    while res not in seen:    \n",
    "        seen[res] = 1\n",
    "        return res\n",
    "    seen[res] = seen[res]+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66edf6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take top 100 US retailers arbitarily chosen based on their annual reported Sales\n",
    "\n",
    "def read_data():\n",
    "    df = pd.read_csv(\"stores.csv\")\n",
    "    del df['empty']\n",
    "    df = df.set_index('no')\n",
    "    df.head()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8e2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will muddle up the stores to simulate real world data capture where system might introduce impurites\n",
    "# the function will generate n number of instances for jumbled data\n",
    "\n",
    "# n indicates number of obfuscated records\n",
    "# instance_count is the number of observation per class\n",
    "\n",
    "def get_obfuscated_stores(stores,n):\n",
    "    obfuscate_stores = stores * n\n",
    "    obfuscate_stores.sort()\n",
    "    df_obfuscated = pd.DataFrame({\"stores\": obfuscate_stores})\n",
    "    df_obfuscated['bad_names'] = df_obfuscated[\"stores\"].apply(lambda x: obfuscate(x) )\n",
    "    return df_obfuscated\n",
    "    \n",
    "def get_catch_gibberish(instance_count,min_l,max_l):\n",
    "    others = ['Other'] * instance_count\n",
    "    df_other = pd.DataFrame({'stores':others})\n",
    "    df_other['bad_names'] = df_other['stores'].apply(lambda x : gen_gibberish(min_l,max_l)) \n",
    "    return df_other\n",
    "\n",
    "# the training set will also have good captures where store name was interpreted correctly\n",
    "# for this purpose the we are creagin 10% bad captures\n",
    "\n",
    "def get_good_names(stores,n):\n",
    "    good_captures = stores*  n\n",
    "    df_good_captures = pd.DataFrame({'stores':good_captures})\n",
    "    df_good_captures['bad_names'] = df_good_captures[\"stores\"]\n",
    "    return df_good_captures\n",
    "\n",
    "def get_data(impute_n,instance,min_l,max_l): \n",
    "    n = impute_n\n",
    "    instance_count = instance\n",
    "    min_l = min_l\n",
    "    max_l = max_l\n",
    "\n",
    "    df_obfuscated = get_obfuscated_stores(stores,n)\n",
    "    df_other = get_catch_gibberish(instance_count,min_l,max_l)\n",
    "    df_good_captures = repeat_good_names(stores,n,instance_count)\n",
    "    df = pd.concat([df_obfuscated,df_good_captures,df_other])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b296c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(split_ratio,impute_ratio,instance,min_l,max_l):\n",
    "    test_size = (instance*split_ratio)\n",
    "    train_size = (instance*(1-split_ratio))\n",
    "    print(f\"test_size = {test_size} & train_size = {train_size} & split = {split_ratio}\")\n",
    "    test_impute_n = int(test_size*impute_ratio)\n",
    "    test_good_n = int(test_size - test_impute_n)\n",
    "    print(f\"test impute = {test_impute_n} & instances = {test_good_n}\")\n",
    "    df = read_data()\n",
    "    stores = list(df[\"store\"])[0:10]\n",
    "    # get imputed\n",
    "    df_obfuscated = get_obfuscated_stores(stores,test_impute_n)\n",
    "    # get good names\n",
    "    df_good_names = get_good_names(stores, test_good_n)\n",
    "    # get others \n",
    "    df_other = get_catch_gibberish(test_impute_n+test_good_n,min_l,max_l)\n",
    "    df_test = pd.concat([df_obfuscated,df_good_names,df_other])\n",
    "    \n",
    "    del df_obfuscated,df_good_names,df_other\n",
    "    train_impute_n = int(train_size*impute_ratio)\n",
    "    train_good_n = int(train_size - train_impute_n)\n",
    "    # get imputed\n",
    "    df_obfuscated = get_obfuscated_stores(stores,train_impute_n)\n",
    "    # get good names\n",
    "    df_good_names = get_good_names(stores, train_good_n)\n",
    "    # get others \n",
    "    df_other = get_catch_gibberish(train_impute_n+train_good_n,min_l,max_l)\n",
    "    \n",
    "    df_train = pd.concat([df_obfuscated,df_good_names,df_other])\n",
    "    \n",
    "    del df_obfuscated,df_good_names,df_other\n",
    "    return df_train, df_test\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "562377f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_size = 2000.0 & train_size = 8000.0 & split = 0.2\n",
      "test impute = 1000 & instances = 1000\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = get_train_test_data(0.2,0.5,10000,5,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f69b1444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len test stores = 11\n",
      "len train stores = 11\n"
     ]
    }
   ],
   "source": [
    "print(f\"len test stores = {len(set(df_test['stores']))}\")\n",
    "print(f\"len train stores = {len(set(df_train['stores']))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "837dc070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac1bf574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "103b2538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad_names</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stores</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Albertsons Companies</th>\n",
       "      <td>4401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon.com</th>\n",
       "      <td>4966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CVS Health Corporation</th>\n",
       "      <td>4378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Costco Wholesale</th>\n",
       "      <td>4561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lowe's Companies</th>\n",
       "      <td>4556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <td>5882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Home Depot</th>\n",
       "      <td>4651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Kroger Co.</th>\n",
       "      <td>4641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walgreens Boots Alliance</th>\n",
       "      <td>4349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walmart</th>\n",
       "      <td>5703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          bad_names\n",
       "stores                             \n",
       "Albertsons Companies           4401\n",
       "Amazon.com                     4966\n",
       "CVS Health Corporation         4378\n",
       "Costco Wholesale               4561\n",
       "Lowe's Companies               4556\n",
       "Target                         5882\n",
       "The Home Depot                 4651\n",
       "The Kroger Co.                 4641\n",
       "Walgreens Boots Alliance       4349\n",
       "Walmart                        5703"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['stores'] == df_train['bad_names']].groupby(\"stores\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1de442f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad_names</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stores</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Albertsons Companies</th>\n",
       "      <td>1083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon.com</th>\n",
       "      <td>1207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CVS Health Corporation</th>\n",
       "      <td>1082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Costco Wholesale</th>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lowe's Companies</th>\n",
       "      <td>1144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <td>1425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Home Depot</th>\n",
       "      <td>1189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Kroger Co.</th>\n",
       "      <td>1166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walgreens Boots Alliance</th>\n",
       "      <td>1069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walmart</th>\n",
       "      <td>1393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          bad_names\n",
       "stores                             \n",
       "Albertsons Companies           1083\n",
       "Amazon.com                     1207\n",
       "CVS Health Corporation         1082\n",
       "Costco Wholesale               1146\n",
       "Lowe's Companies               1144\n",
       "Target                         1425\n",
       "The Home Depot                 1189\n",
       "The Kroger Co.                 1166\n",
       "Walgreens Boots Alliance       1069\n",
       "Walmart                        1393"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test['stores'] == df_test['bad_names']].groupby(\"stores\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60f44509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stores       2000\n",
       "bad_names    2000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test[\"stores\"] == 'Other'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2ced183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stores       8000\n",
       "bad_names    8000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train[\"stores\"] == 'Other'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39b9ad1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stores</th>\n",
       "      <th>bad_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Albe%rts1ons C7,2'om1 pan8-ie!s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>2 Albertsons C4'ompanie3s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Albertsons Com4?pani,es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>5?*Albe-rts(ons Companies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Albertsons$ Com3)$pa&lt;nies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>6$Albertsons Com4&gt;panies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Albertsons#!. Companies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Al&amp;bert.&amp;sons Companies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Al%bert(sons Compani-es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Alberts9ons C&amp;ompa%?nies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 stores                        bad_names\n",
       "0  Albertsons Companies  Albe%rts1ons C7,2'om1 pan8-ie!s\n",
       "1  Albertsons Companies        2 Albertsons C4'ompanie3s\n",
       "2  Albertsons Companies          Albertsons Com4?pani,es\n",
       "3  Albertsons Companies        5?*Albe-rts(ons Companies\n",
       "4  Albertsons Companies        Albertsons$ Com3)$pa<nies\n",
       "5  Albertsons Companies         6$Albertsons Com4>panies\n",
       "6  Albertsons Companies          Albertsons#!. Companies\n",
       "7  Albertsons Companies          Al&bert.&sons Companies\n",
       "8  Albertsons Companies          Al%bert(sons Compani-es\n",
       "9  Albertsons Companies         Alberts9ons C&ompa%?nies"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd361585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stores</th>\n",
       "      <th>bad_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>7!Albertsons Compani+e's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>3(Albertsons Companies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Al&lt;bertsons Compa nies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Albe&lt;rtson&lt;s Compan'i$es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Al%bert-sons Comp?anies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Al^bertsons Companie/s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Albertso ns Comp!ani+)@,es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Al&amp;be&lt;rtsons Compani!es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Albe)rtsons Comp#a&gt;%n ies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Albe)rt1&gt;sons Compan%ies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 stores                   bad_names\n",
       "0  Albertsons Companies    7!Albertsons Compani+e's\n",
       "1  Albertsons Companies      3(Albertsons Companies\n",
       "2  Albertsons Companies      Al<bertsons Compa nies\n",
       "3  Albertsons Companies    Albe<rtson<s Compan'i$es\n",
       "4  Albertsons Companies     Al%bert-sons Comp?anies\n",
       "5  Albertsons Companies      Al^bertsons Companie/s\n",
       "6  Albertsons Companies  Albertso ns Comp!ani+)@,es\n",
       "7  Albertsons Companies     Al&be<rtsons Compani!es\n",
       "8  Albertsons Companies   Albe)rtsons Comp#a>%n ies\n",
       "9  Albertsons Companies    Albe)rt1>sons Compan%ies"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ff7e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "def get_train_test_split(df_train,df_test,one_hot_encode_labels=False):\n",
    "    X_train = df_train[\"bad_names\"].values\n",
    "    y_train = df_train[\"stores\"].values\n",
    "    X_test  = df_test[\"bad_names\"].values\n",
    "    y_test  = df_test[\"stores\"].values\n",
    "    \n",
    "    if one_hot_encode_labels:\n",
    "        df_labels = pd.concat([df_train['stores'],df_test['stores']])\n",
    "        print(f\"Labels = {df_labels.shape})\")\n",
    "        lables = pd.get_dummies(df_labels)\n",
    "        lookup = list(lables.columns)\n",
    "        print(len(lookup))\n",
    "        del df_labels\n",
    "        y_test_labels = pd.get_dummies(y_test)\n",
    "        y_test_encoded = y_test_labels.astype('float32').values\n",
    "        y_train_labels = pd.get_dummies(y_train)\n",
    "        y_train_encoded = y_train_labels.astype('float32').values\n",
    "        return X_train,y_train_encoded,X_test,y_test_encoded, lookup\n",
    "    else:\n",
    "        return X_train, y_train, X_test, y_test, None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bad3a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, y_train, X_test, y_test, lable_lookup = get_train_test_split(df_train,\n",
    "                                                                      df_test,\n",
    "                                                                      one_hot_encode_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b7d27a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88000,)\n",
      "(88000,)\n",
      "(22000,)\n",
      "(22000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9df1f890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "39922\n"
     ]
    }
   ],
   "source": [
    "print(len(set(y_train)))\n",
    "print(len(set(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02b94421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Walmart',\n",
       " 'CVS Health Corporation',\n",
       " 'The Home Depot',\n",
       " 'Walgreens Boots Alliance',\n",
       " \"Lowe's Companies\",\n",
       " 'Amazon.com',\n",
       " 'Costco Wholesale',\n",
       " 'Target',\n",
       " 'Other',\n",
       " 'The Kroger Co.',\n",
       " 'Albertsons Companies']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(set(y_train))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "922f3eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_index(label,labels=labels):\n",
    "    return labels.index(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eabbeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_labels(idx,labels=labels):\n",
    "    return labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lables(pred,lookup):\n",
    "    return lookup[np.argmax(pred)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d85c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_encode_labels = False\n",
    "# if one_hot_encode_labels:\n",
    "#     for i in range(5):\n",
    "#         print(f\" TRAIN : {X_train[i]} >  {get_lables(np.array(y_train[i]),lable_lookup)}\")\n",
    "#     for i in range(5):\n",
    "#         print(f\" TEST : {X_test[i]} >  {get_lables(np.array(y_test[i]),lable_lookup)}\")\n",
    "# else:\n",
    "#     for i in range(5):\n",
    "#         print(f\" TRAIN : {X_train[i]} >  {y_train[i]}\")\n",
    "#     for i in range(5):\n",
    "#         print(f\" TEST : {X_test[i]} >  {y_test[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8865ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab = ['UNK']\n",
    "# Vocab = Vocab+ list(set(\"\".join(X_train)))\n",
    "# print(len(Vocab))\n",
    "# #Vocab.append(\"UNK\")\n",
    "# char_to_ind = {u:i for i, u in enumerate(Vocab)}\n",
    "# vocab_len = len(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2fb2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tx = [labels_to_index(y) for y in y_train]\n",
    "y_test_tx = [labels_to_index(y) for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15266f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_to_char = np.array(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tx = []\n",
    "for x in X_train:\n",
    "    X_train_tx.append([char_to_ind[c] for c in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tx = []\n",
    "for x in X_test:\n",
    "    X_test_tx.append([char_to_ind[c] for c in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tx = []\n",
    "for y in y_train:\n",
    "    y_train_tx.append([char_to_ind[c] for c in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f86b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tx = []\n",
    "for y in y_test:\n",
    "    y_test_tx.append([char_to_ind[c] for c in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6018b1",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_word_from_ind(arr):\n",
    "    return \"\".join([ind_to_char[i] for i in arr]).replace(\"UNK\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dda2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_word_from_ind(X_train_tx[0]))\n",
    "print(get_word_from_ind(X_test_tx[0]))\n",
    "print(get_word_from_ind(y_train_tx[0]))\n",
    "print(get_word_from_ind(y_train_tx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42908782",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_len= 0\n",
    "for i in X_train:\n",
    "    if len(i)>x_max_len:\n",
    "        x_max_len= len(i)\n",
    "print(x_max_len)\n",
    "x_max_input_length = x_max_len\n",
    "\n",
    "x_min_len = np.inf\n",
    "for i in X_train:\n",
    "    if len(i)<x_min_len:\n",
    "        x_min_len = len(i)\n",
    "print(x_min_len)\n",
    "min_input_length = x_min_len\n",
    "\n",
    "y_min_len = np.inf\n",
    "for i in y_train:\n",
    "    if len(i)<y_min_len:\n",
    "        y_min_len = len(i)\n",
    "print(y_min_len)\n",
    "\n",
    "y_max_len = 0\n",
    "for i in y_train:\n",
    "    if len(i)>y_max_len:\n",
    "        y_max_len = len(i)\n",
    "print(y_max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length =24\n",
    "def gen_encoded_strings(word,input_len):\n",
    "    word_len = len(word)\n",
    "    if word_len > input_len:\n",
    "        word = word[0:input_len]\n",
    "        return [char_to_ind[c] for c in word]\n",
    "    else:\n",
    "        encoded = [char_to_ind[c] for c in word]\n",
    "        pad_width = input_len-word_len\n",
    "        arr = np.array(encoded)\n",
    "        return np.pad(arr,(pad_width,0),\"constant\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb07c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the encoded and decoded.\n",
    "\n",
    "s = 'Target'\n",
    "\n",
    "s_encoded = gen_encoded_strings(s,input_length)\n",
    "\n",
    "print(f\" encoded - {s_encoded} - {len(s_encoded)}\")\n",
    "\n",
    "print(f\" decoded - {get_word_from_ind(s_encoded)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb2b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = list(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def categorical_coding(labels):\n",
    "#     idx = labels.index(label)\n",
    "#     return [idx]\n",
    "\n",
    "# def categorical_label(idx):\n",
    "#     return labels[idx[0]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0])\n",
    "#print(categorical_coding(y_train[0]))\n",
    "#print(categorical_label(categorical_coding(y_train[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_features = 100\n",
    "# vectorize_layer = layers.TextVectorization(\n",
    "#     standardize=\"lower\",\n",
    "#     split='character',\n",
    "#     output_sequence_length=max_features,\n",
    "#     max_tokens=max_features,\n",
    "#     pad_to_max_tokens=True,\n",
    "#     output_mode='int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be878571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize_layer.adapt(X_train)\n",
    "# len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db3b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode both label and test\n",
    "# def vectorize_text_code_lable(text, label):\n",
    "#     vector = gen_encoded_strings(text,input_length)\n",
    "#     idx = categorical_coding(label)\n",
    "#     return vector, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize_layer.adapt(\n",
    "#          X_train\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae13ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_categorical_coding = np.vectorize(categorical_coding)\n",
    "# vector_gen_encoded_strings = np.vectorize(gen_encoded_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5480812",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = [gen_encoded_strings(x,input_length) for x in X_train]\n",
    "print(f\" type - {type(X_train_encoded)}\")\n",
    "X_test_encoded = [gen_encoded_strings(x,input_length) for x in X_test]\n",
    "print(f\" type - {type(X_test_encoded)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ae1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = [gen_encoded_strings(y,input_length) for y in y_train]\n",
    "print(f\" type - {type(y_train_encoded)}\")\n",
    "y_test_encoded = [gen_encoded_strings(y,input_length) for y in y_test]\n",
    "print(f\" type - {type(y_test_encoded)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_encoded = [categorical_coding(y) for y in y_train]\n",
    "# type(f\"type = {type(y_train_encoded)}\")\n",
    "# y_test_encoded = [categorical_coding(y) for y in y_test]\n",
    "# type(f\"type = {type(y_test_encoded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a3ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded[0]\n",
    "X_test_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4798c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X_train_encoded:\n",
    "    if len(x) != 24:\n",
    "        print(len(x))\n",
    "        \n",
    "for y in y_train_encoded:\n",
    "    if len(y) != 24:\n",
    "        print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y_train_encoded[0]))\n",
    "print(type(y_test_encoded[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe51f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((X_train_encoded, y_train_encoded))\n",
    "val_data = tf.data.Dataset.from_tensor_slices((X_test_encoded, y_test_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab5051",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #lable_lookup\n",
    "# def map_labels(x,y):\n",
    "#     print(f\"y - {type(y)} - {y.shape}\")\n",
    "#     print(y[0])\n",
    "#     l = get_lables(y[0],lable_lookup)\n",
    "#     encoded = gen_encoded_strings(l,input_length)\n",
    "#     (encoded,y)\n",
    "#     return x,(encoded,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecadcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for example, label in train_data.take(1):\n",
    "#     print('text: ', example.numpy())\n",
    "#     print('label: ', label.numpy())\n",
    "#     x,y = map_labels(example.numpy(),label)\n",
    "#     #print('encoded label: ', y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize_text_code_lable('Target','Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba41e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 14000\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "train_dataset = train_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_data.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315eb9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a5702",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "    print(f\"{example.shape} - {label.shape}\")\n",
    "    #print('text: ', example.numpy()[:3])\n",
    "    #print('label_one_hot: ', label.numpy()[:3])\n",
    "    #for v in label.numpy()[:3]:\n",
    "        #print('label ', decode_pred(v,lookup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = np.array(vectorize_layer.get_vocabulary())\n",
    "# vocab_len = len(vocab)\n",
    "# print(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee7ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_dataset.take(1):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example=\"Target\"\n",
    "# encoded_example = vectorize_layer(example).numpy()\n",
    "# encoded_example.shape\n",
    "len(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 20\n",
    "batch_size = 512\n",
    "time_steps = 512\n",
    "\n",
    "def get_model(embedding_dim,batch_size,time_steps):\n",
    "    model = tf.keras.Sequential()\n",
    "    embed_layer = layers.Embedding(\n",
    "                       input_dim=len(Vocab), # e.g, 10 if you have 10 words in your vocabulary\n",
    "                       output_dim=embedding_dim, # size of the embedded vectors\n",
    "                       batch_input_shape = [batch_size,None]\n",
    "                        )\n",
    "    bidirect_lstm = layers.Bidirectional(layers.LSTM(\n",
    "                                        units=time_steps,\n",
    "                                        return_sequences=True,\n",
    "                                        stateful=True,\n",
    "                                        activation='tanh',\n",
    "                                        recurrent_activation=\"tanh\", \n",
    "                                        recurrent_initializer='glorot_uniform'))\n",
    "    pooling1 = layers.GlobalAveragePooling1D()\n",
    "    dense1 = layers.Dense(96, activation = 'relu', kernel_regularizer='l2')\n",
    "    drop1 = layers.Dropout(0.2)\n",
    "    dense2 = layers.Dense(128, activation = 'tanh', kernel_regularizer='l2')\n",
    "    drop2 = layers.Dropout(0.2)\n",
    "    dense3 = layers.Dense(256, activation = 'tanh')\n",
    "    drop3 = layers.Dropout(0.2)\n",
    "    dense4 = layers.Dense(128, activation = 'sigmoid', kernel_regularizer='l2')\n",
    "    drop4 = layers.Dropout(0.2)\n",
    "    predictor = layers.Dense(24)\n",
    "    for l in [embed_layer,\n",
    "              bidirect_lstm,\n",
    "              dense1,\n",
    "              drop1,\n",
    "              dense2,\n",
    "              drop2,\n",
    "              pooling1,\n",
    "              predictor]:\n",
    "        model.add(l)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59cce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(embedding_dim,batch_size,time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([layer.supports_masking for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a829d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(vocab))\n",
    "# sample_text = vectorize_layer('WALMART 1')\n",
    "# #print(sample_text.shape)\n",
    "# #print(type(sample_text))\n",
    "# #print(np.array(sample_text))\n",
    "# sample = np.array(sample_text).reshape(1,len(vocab))\n",
    "# print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70641e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "#import tf.keras.optimizers.experimental.Adadelta\n",
    "# opt = SGD(learning_rate=0.01)\n",
    "# opt = Optimizer()\n",
    "def cat_loss(y_true,y_pred):\n",
    "    pred = tf.reshape(y_pred[0][0],(1,101))\n",
    "    return categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "loss_fn = 'categorical_crossentropy'\n",
    "#loss_fn = cat_loss\n",
    "\n",
    "model.compile(loss=loss_fn,\n",
    "              optimizer= 'adam',\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045c1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_loss',patience=5)\n",
    "# from tensorflow.python.compiler.mlcompute import mlcompute\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "# mlcompute.set_mlc_device(device_name='gpu')\n",
    "# print(\"is_apple_mlc_enabled %s\" % mlcompute.is_apple_mlc_enabled())\n",
    "# print(\"is_tf_compiled_with_apple_mlc %s\" % mlcompute.is_tf_compiled_with_apple_mlc())\n",
    "# print(f\"eagerly? {tf.executing_eagerly()}\")\n",
    "print(tf.config.list_logical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a10368",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(train_dataset, \n",
    "                epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9dd28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = get_model(embedding_dim,1,time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b1b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b82ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de466dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pred_model.predict(np.array(x[0]).reshape(1,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9183c",
   "metadata": {},
   "outputs": [],
   "source": [
    " predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions/1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf6ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_id = tf.random.categorical(predictions, num_samples =24) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431adfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [decode_pred(i,lookup) for i in y_train]\n",
    "\n",
    "df_train = pd.DataFrame({'stores':labels, 'bad_names':X_train})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f3fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('stores').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name = \"WALMART\"\n",
    "\n",
    "v_store_name = vectorize_text(store_name,'unknown')\n",
    "#print(v_store_name[0].shape)\n",
    "print(np.array(v_store_name[0]).reshape(1,3000))\n",
    "#data = tf.data.Dataset.from_tensor_slices(v_store_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa5e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_pred(np.argmax(model.predict(np.array(v_store_name[0]).reshape(1,3000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7bc3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.10 (ml)",
   "language": "python",
   "name": "ml-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
