{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a function to oobfuscate the store names and introduce impurities for the sake of this excercise\n",
    "# Scrapped or OCR results could be much more random and unpredictable.\n",
    "import string\n",
    "import random \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "seen = {}\n",
    "symbols = \"!@#$%^&*()-+,<.>?/'' \"\n",
    "numbers = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "\n",
    "def gen_impurities(n,word):\n",
    "    for i in range(n):\n",
    "        index = random.randint(0,len(word)-1)\n",
    "        #print(f\"impute index - {index}\")\n",
    "\n",
    "        if index%2 == 0:\n",
    "            symbol = random.choice(symbols)\n",
    "            #print(f\"Symbol = {symbol}\")\n",
    "            word = word[:index] + symbol + word[index:]\n",
    "\n",
    "        if index%7 == 0:\n",
    "            num = random.choice(numbers)\n",
    "            #print(f\"Number = {num}\")\n",
    "            word = word[:index] + str(num) + word[index:]\n",
    "\n",
    "        if index%3 == 0:\n",
    "            word.replace(word[index],\"\")\n",
    "    return word\n",
    "\n",
    "def obfuscate(word):\n",
    "    n = int(len(word)*0.5)\n",
    "#     print(n)\n",
    "    iteration = random.randint(1,n)\n",
    "#     print(iteration)\n",
    "    label = word\n",
    "    for i in range(5):\n",
    "        imputed_word = gen_impurities(iteration,label)\n",
    "#         print(imputed_word)\n",
    "        if imputed_word not in seen:\n",
    "            seen[imputed_word] = 1\n",
    "            return imputed_word\n",
    "        seen[imputed_word] = seen[imputed_word]+1\n",
    "    return word\n",
    "            \n",
    "\n",
    "def gen_gibberish(min_l,max_l):\n",
    "    # initializing size of string\n",
    "    l = random.randint(min_l,max_l)\n",
    "    # using random.choices()\n",
    "    # generating random strings\n",
    "    res = ''.join(random.choices(string.ascii_uppercase +\n",
    "                                 string.punctuation +\n",
    "                                 string.digits, k=l))\n",
    "    while res not in seen:    \n",
    "        seen[res] = 1\n",
    "        return res\n",
    "    seen[res] = seen[res]+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66edf6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take top 100 US retailers arbitarily chosen based on their annual reported Sales\n",
    "\n",
    "def read_data():\n",
    "    df = pd.read_csv(\"stores.csv\")\n",
    "    del df['empty']\n",
    "    df = df.set_index('no')\n",
    "    df.head()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will muddle up the stores to simulate real world data capture where system might introduce impurites\n",
    "# the function will generate n number of instances for jumbled data\n",
    "\n",
    "# n indicates number of obfuscated records\n",
    "# instance_count is the number of observation per class\n",
    "\n",
    "def get_obfuscated_stores(stores,n):\n",
    "    obfuscate_stores = stores * n\n",
    "    obfuscate_stores.sort()\n",
    "    df_obfuscated = pd.DataFrame({\"stores\": obfuscate_stores})\n",
    "    df_obfuscated['bad_names'] = df_obfuscated[\"stores\"].apply(lambda x: obfuscate(x) )\n",
    "    return df_obfuscated\n",
    "    \n",
    "def get_catch_gibberish(instance_count,min_l,max_l):\n",
    "    others = ['Other'] * instance_count\n",
    "    df_other = pd.DataFrame({'stores':others})\n",
    "    df_other['bad_names'] = df_other['stores'].apply(lambda x : gen_gibberish(min_l,max_l)) \n",
    "    return df_other\n",
    "\n",
    "# the training set will also have good captures where store name was interpreted correctly\n",
    "# for this purpose the we are creagin 10% bad captures\n",
    "\n",
    "def get_good_names(stores,n):\n",
    "    good_captures = stores*  n\n",
    "    df_good_captures = pd.DataFrame({'stores':good_captures})\n",
    "    df_good_captures['bad_names'] = df_good_captures[\"stores\"]\n",
    "    return df_good_captures\n",
    "\n",
    "def get_data(impute_n,instance,min_l,max_l): \n",
    "    n = impute_n\n",
    "    instance_count = instance\n",
    "    min_l = min_l\n",
    "    max_l = max_l\n",
    "\n",
    "    df_obfuscated = get_obfuscated_stores(stores,n)\n",
    "    df_other = get_catch_gibberish(instance_count,min_l,max_l)\n",
    "    df_good_captures = repeat_good_names(stores,n,instance_count)\n",
    "    df = pd.concat([df_obfuscated,df_good_captures,df_other])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b296c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(split_ratio,impute_ratio,instance,min_l,max_l):\n",
    "    test_size = (instance*split_ratio)\n",
    "    train_size = (instance*(1-split_ratio))\n",
    "    print(f\"test_size = {test_size} & train_size = {train_size} & split = {split_ratio}\")\n",
    "    test_impute_n = int(test_size*impute_ratio)\n",
    "    test_good_n = int(test_size - test_impute_n)\n",
    "    print(f\"test impute = {test_impute_n} & instances = {test_good_n}\")\n",
    "    df = read_data()\n",
    "    stores = list(df[\"store\"])[0:10]\n",
    "    # get imputed\n",
    "    df_obfuscated = get_obfuscated_stores(stores,test_impute_n)\n",
    "    # get good names\n",
    "    df_good_names = get_good_names(stores, test_good_n)\n",
    "    # get others \n",
    "    df_other = get_catch_gibberish(test_impute_n+test_good_n,min_l,max_l)\n",
    "    df_test = pd.concat([df_obfuscated,df_good_names,df_other])\n",
    "    \n",
    "    del df_obfuscated,df_good_names,df_other\n",
    "    train_impute_n = int(train_size*impute_ratio)\n",
    "    train_good_n = int(train_size - train_impute_n)\n",
    "    # get imputed\n",
    "    df_obfuscated = get_obfuscated_stores(stores,train_impute_n)\n",
    "    # get good names\n",
    "    df_good_names = get_good_names(stores, train_good_n)\n",
    "    # get others \n",
    "    df_other = get_catch_gibberish(train_impute_n+train_good_n,min_l,max_l)\n",
    "    \n",
    "    df_train = pd.concat([df_obfuscated,df_good_names,df_other])\n",
    "    \n",
    "    del df_obfuscated,df_good_names,df_other\n",
    "    return df_train, df_test\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562377f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = get_train_test_data(0.2,0.5,10000,5,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len test stores = {len(set(df_test['stores']))}\")\n",
    "print(f\"len train stores = {len(set(df_train['stores']))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837dc070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1bf574",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['stores'] == df_train['bad_names']].groupby(\"stores\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test['stores'] == df_test['bad_names']].groupby(\"stores\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f44509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test[\"stores\"] == 'Other'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ced183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train[\"stores\"] == 'Other'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9ad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd361585",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff7e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "def get_train_test_split(df_train,df_test,one_hot_encode_labels=False):\n",
    "    X_train = df_train[\"bad_names\"].values\n",
    "    y_train = df_train[\"stores\"].values\n",
    "    X_test  = df_test[\"bad_names\"].values\n",
    "    y_test  = df_test[\"stores\"].values\n",
    "    \n",
    "    if one_hot_encode_labels:\n",
    "        df_labels = pd.concat([df_train['stores'],df_test['stores']])\n",
    "        print(f\"Labels = {df_labels.shape})\")\n",
    "        lables = pd.get_dummies(df_labels)\n",
    "        lookup = list(lables.columns)\n",
    "        print(len(lookup))\n",
    "        del df_labels\n",
    "        y_test_labels = pd.get_dummies(y_test)\n",
    "        y_test_encoded = y_test_labels.astype('float32').values\n",
    "        y_train_labels = pd.get_dummies(y_train)\n",
    "        y_train_encoded = y_train_labels.astype('float32').values\n",
    "        return X_train,y_train_encoded,X_test,y_test_encoded, lookup\n",
    "    else:\n",
    "        return X_train, y_train, X_test, y_test, None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, y_train, X_test, y_test, lable_lookup = get_train_test_split(df_train,\n",
    "                                                                      df_test,\n",
    "                                                                      one_hot_encode_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df1f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(y_train)))\n",
    "print(len(set(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b94421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "labels = list(set(y_train))\n",
    "# enc = OneHotEncoder(handle_unknown = 'ignore')\n",
    "# enc.fit(np.array(labels).reshape(-1,1))\n",
    "# y_train_tx = enc.transform(np.array(y_train).reshape(-1,1)).toarray()\n",
    "# y_test_tx = enc.transform(np.array(y_test).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.inverse_transform(test[0].reshape(-1,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9063367",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.argmax(test[0])} - {y_train[0]} - {labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d292565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_index(label,labels=labels):\n",
    "    return labels.index(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_labels(idx,labels=labels):\n",
    "    return labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lables(pred,lookup):\n",
    "    return lookup[np.argmax(pred)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d85c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_encode_labels = False\n",
    "# if one_hot_encode_labels:\n",
    "#     for i in range(5):\n",
    "#         print(f\" TRAIN : {X_train[i]} >  {get_lables(np.array(y_train[i]),lable_lookup)}\")\n",
    "#     for i in range(5):\n",
    "#         print(f\" TEST : {X_test[i]} >  {get_lables(np.array(y_test[i]),lable_lookup)}\")\n",
    "# else:\n",
    "#     for i in range(5):\n",
    "#         print(f\" TRAIN : {X_train[i]} >  {y_train[i]}\")\n",
    "#     for i in range(5):\n",
    "#         print(f\" TEST : {X_test[i]} >  {y_test[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8865ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab = ['UNK']\n",
    "# Vocab = Vocab+ list(set(\"\".join(X_train)))\n",
    "# print(len(Vocab))\n",
    "# #Vocab.append(\"UNK\")\n",
    "# char_to_ind = {u:i for i, u in enumerate(Vocab)}\n",
    "# vocab_len = len(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ac37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tx = [labels_to_index(y) for y in y_train]\n",
    "y_test_tx = [labels_to_index(y) for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf43960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_tx[0])\n",
    "print(y_train[0])\n",
    "print((y_test_tx)[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind_to_char = np.array(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_tx = []\n",
    "# for x in X_train:\n",
    "#     X_train_tx.append([char_to_ind[c] for c in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_tx = []\n",
    "# for x in X_test:\n",
    "#     X_test_tx.append([char_to_ind[c] for c in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_tx = []\n",
    "# for y in y_train:\n",
    "#     y_train_tx.append([char_to_ind[c] for c in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f86b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_tx = []\n",
    "# for y in y_test:\n",
    "#     y_test_tx.append([char_to_ind[c] for c in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6018b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def get_word_from_ind(arr):\n",
    "#     return \"\".join([ind_to_char[i] for i in arr]).replace(\"UNK\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dda2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_word_from_ind(X_train_tx[0]))\n",
    "# print(get_word_from_ind(X_test_tx[0]))\n",
    "# print(get_word_from_ind(y_train_tx[0]))\n",
    "# print(get_word_from_ind(y_train_tx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42908782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_max_len= 0\n",
    "# for i in X_train:\n",
    "#     if len(i)>x_max_len:\n",
    "#         x_max_len= len(i)\n",
    "# print(x_max_len)\n",
    "# x_max_input_length = x_max_len\n",
    "\n",
    "# x_min_len = np.inf\n",
    "# for i in X_train:\n",
    "#     if len(i)<x_min_len:\n",
    "#         x_min_len = len(i)\n",
    "# print(x_min_len)\n",
    "# min_input_length = x_min_len\n",
    "\n",
    "# y_min_len = np.inf\n",
    "# for i in y_train:\n",
    "#     if len(i)<y_min_len:\n",
    "#         y_min_len = len(i)\n",
    "# print(y_min_len)\n",
    "\n",
    "# y_max_len = 0\n",
    "# for i in y_train:\n",
    "#     if len(i)>y_max_len:\n",
    "#         y_max_len = len(i)\n",
    "# print(y_max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_length =24\n",
    "# def gen_encoded_strings(word,input_len):\n",
    "#     word_len = len(word)\n",
    "#     if word_len > input_len:\n",
    "#         word = word[0:input_len]\n",
    "#         return [char_to_ind[c] for c in word]\n",
    "#     else:\n",
    "#         encoded = [char_to_ind[c] for c in word]\n",
    "#         pad_width = input_len-word_len\n",
    "#         arr = np.array(encoded)\n",
    "#         return np.pad(arr,(pad_width,0),\"constant\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb07c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing the encoded and decoded.\n",
    "\n",
    "# s = 'Target'\n",
    "\n",
    "# s_encoded = gen_encoded_strings(s,input_length)\n",
    "\n",
    "# print(f\" encoded - {s_encoded} - {len(s_encoded)}\")\n",
    "\n",
    "# print(f\" decoded - {get_word_from_ind(s_encoded)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb2b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = list(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def categorical_coding(labels):\n",
    "#     idx = labels.index(label)\n",
    "#     return [idx]\n",
    "\n",
    "# def categorical_label(idx):\n",
    "#     return labels[idx[0]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0])\n",
    "#print(categorical_coding(y_train[0]))\n",
    "#print(categorical_label(categorical_coding(y_train[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_features = 100\n",
    "# vectorize_layer = layers.TextVectorization(\n",
    "#     standardize=\"lower\",\n",
    "#     split='character',\n",
    "#     output_sequence_length=max_features,\n",
    "#     max_tokens=max_features,\n",
    "#     pad_to_max_tokens=True,\n",
    "#     output_mode='int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be878571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize_layer.adapt(X_train)\n",
    "# len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db3b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode both label and test\n",
    "# def vectorize_text_code_lable(text, label):\n",
    "#     vector = gen_encoded_strings(text,input_length)\n",
    "#     idx = categorical_coding(label)\n",
    "#     return vector, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize_layer.adapt(\n",
    "#          X_train\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae13ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_categorical_coding = np.vectorize(categorical_coding)\n",
    "# vector_gen_encoded_strings = np.vectorize(gen_encoded_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5480812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_encoded = [gen_encoded_strings(x,input_length) for x in X_train]\n",
    "# print(f\" type - {type(X_train_encoded)}\")\n",
    "# X_test_encoded = [gen_encoded_strings(x,input_length) for x in X_test]\n",
    "# print(f\" type - {type(X_test_encoded)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ae1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_encoded = [gen_encoded_strings(y,input_length) for y in y_train]\n",
    "# print(f\" type - {type(y_train_encoded)}\")\n",
    "# y_test_encoded = [gen_encoded_strings(y,input_length) for y in y_test]\n",
    "# print(f\" type - {type(y_test_encoded)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_encoded = [categorical_coding(y) for y in y_train]\n",
    "# type(f\"type = {type(y_train_encoded)}\")\n",
    "# y_test_encoded = [categorical_coding(y) for y in y_test]\n",
    "# type(f\"type = {type(y_test_encoded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a3ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_encoded[0]\n",
    "# X_test_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4798c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in X_train_encoded:\n",
    "#     if len(x) != 24:\n",
    "#         print(len(x))\n",
    "        \n",
    "# for y in y_train_encoded:\n",
    "#     if len(y) != 24:\n",
    "#         print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(y_train_encoded[0]))\n",
    "# print(type(y_test_encoded[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe51f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = tf.data.Dataset.from_tensor_slices((X_train, y_train_tx))\n",
    "size = len(raw_train)\n",
    "raw_train_ds = raw_train.take(int(size*0.8))\n",
    "raw_val_ds = raw_train.take(int(size*0.2))\n",
    "raw_test = tf.data.Dataset.from_tensor_slices((X_test, y_test_tx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab5051",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb06e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c96f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100\n",
    "sequence_length = 30\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    split = 'character',\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62cb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = raw_train.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30be455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text)[0], label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, label_batch = next(iter(raw_train))\n",
    "#first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"text\", text_batch)\n",
    "print(\"Label\",label_batch)\n",
    "print(\"Vectorized text\", vectorize_text(text_batch, label_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #lable_lookup\n",
    "# def map_labels(x,y):\n",
    "#     print(f\"y - {type(y)} - {y.shape}\")\n",
    "#     print(y[0])\n",
    "#     l = get_lables(y[0],lable_lookup)\n",
    "#     encoded = gen_encoded_strings(l,input_length)\n",
    "#     (encoded,y)\n",
    "#     return x,(encoded,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecadcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for example, label in train_data.take(1):\n",
    "#     print('text: ', example.numpy())\n",
    "#     print('label: ', label.numpy())\n",
    "#     x,y = map_labels(example.numpy(),label)\n",
    "#     #print('encoded label: ', y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize_text_code_lable('Target','Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a605a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3 ---> \",vectorize_layer.get_vocabulary()[3])\n",
    "print(\" 30 ---> \",vectorize_layer.get_vocabulary()[30])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba41e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 200\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "#AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.shuffle(buffer_size=BUFFER_SIZE).batch(batch_size=BATCH_SIZE,drop_remainder=True)\n",
    "val_ds = val_ds.batch(batch_size=BATCH_SIZE,drop_remainder=True)\n",
    "test_ds = test_ds.batch(batch_size=BATCH_SIZE,drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d277ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_ds.take(1):\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a5702",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "    print(f\"{example.shape} - {label.shape}\")\n",
    "    #print('text: ', example.numpy()[:3])\n",
    "    #print('label_one_hot: ', label.numpy()[:3])\n",
    "    #for v in label.numpy()[:3]:\n",
    "        #print('label ', decode_pred(v,lookup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = np.array(vectorize_layer.get_vocabulary())\n",
    "# vocab_len = len(vocab)\n",
    "# print(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee7ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,y in train_dataset.take(1):\n",
    "#     print(x.shape)\n",
    "#     print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example=\"Target\"\n",
    "# encoded_example = vectorize_layer(example).numpy()\n",
    "# encoded_example.shape\n",
    "len(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5af03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "\n",
    "def get_model(embedding_dim,max_features):\n",
    "    model = tf.keras.Sequential([\n",
    "          layers.Embedding(max_features + 1, embedding_dim),\n",
    "          layers.Bidirectional(layers.LSTM(128)),\n",
    "          #layers.Dropout(0.2),\n",
    "          #layers.GlobalAveragePooling1D(),\n",
    "          layers.Dropout(0.2),\n",
    "          layers.Dense(11, activation='softmax')])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59cce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(embedding_dim,max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([layer.supports_masking for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a829d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(vocab))\n",
    "# sample_text = vectorize_layer('WALMART 1')\n",
    "# #print(sample_text.shape)\n",
    "# #print(type(sample_text))\n",
    "# #print(np.array(sample_text))\n",
    "# sample = np.array(sample_text).reshape(1,len(vocab))\n",
    "# print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70641e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer= 'adam',\n",
    "              metrics=tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045c1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_loss',patience=5)\n",
    "# from tensorflow.python.compiler.mlcompute import mlcompute\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "# mlcompute.set_mlc_device(device_name='gpu')\n",
    "# print(\"is_apple_mlc_enabled %s\" % mlcompute.is_apple_mlc_enabled())\n",
    "# print(\"is_tf_compiled_with_apple_mlc %s\" % mlcompute.is_tf_compiled_with_apple_mlc())\n",
    "# print(f\"eagerly? {tf.executing_eagerly()}\")\n",
    "print(tf.config.list_logical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a10368",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9dd28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = get_model(embedding_dim,max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b1b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b82ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=tf.keras.metrics.SparseCategoricalAccuracy(), \n",
    "    optimizer=\"adam\",\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "loss, accuracy , x= export_model.evaluate(test_ds)\n",
    "print(accuracy)\n",
    "print(loss)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f770259a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predict_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m      2\u001b[0m   vectorize_layer,\n\u001b[1;32m      3\u001b[0m   model,\n\u001b[1;32m      4\u001b[0m   layers\u001b[38;5;241m.\u001b[39mActivation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m ])\n\u001b[1;32m      8\u001b[0m Test_batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWAL1MART\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT163TAS\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m predict_model\u001b[38;5;241m.\u001b[39mpredict(Test_batch)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "predict_model = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  model,\n",
    "  layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "Test_batch = ['WAL1MART','T163TAS']\n",
    "\n",
    "predict_model.predict(Test_batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f691326",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf6ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict_model.predict(Test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431adfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_labels(np.argmax(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f3fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_string(text:str):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640787c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa5e59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7bc3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.10 (ml)",
   "language": "python",
   "name": "ml-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
