{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1068e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a function to oobfuscate the store names and introduce impurities for the sake of this excercise\n",
    "# Scrapped or OCR results could be much more random and unpredictable.\n",
    "import string\n",
    "import random \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Using seen dictionary to generate all unique values\n",
    "# Main goal here is to ensure there is no cross category contamination\n",
    "# 1 input has single label, that model does not get confused\n",
    "seen = {}\n",
    "\n",
    "def gen_impurities(n,word):\n",
    "    for i in range(n):\n",
    "        index = random.randint(0,len(word)-1)\n",
    "        #print(f\"impute index - {index}\")\n",
    "\n",
    "        if index%2 == 0:\n",
    "            symbol = random.choice(string.punctuation)\n",
    "            #print(f\"Symbol = {symbol}\")\n",
    "            word = word[:index] + symbol + word[index:]\n",
    "\n",
    "        if index%7 == 0:\n",
    "            num = random.choice(string.digits)\n",
    "            #print(f\"Number = {num}\")\n",
    "            word = word[:index] + str(num) + word[index:]\n",
    "\n",
    "        if index%3 == 0:\n",
    "            word.replace(word[index],\"\")\n",
    "    return word\n",
    "\n",
    "def obfuscate(word):\n",
    "    n = int(len(word)*0.5)\n",
    "#     print(n)\n",
    "    iteration = random.randint(1,n)\n",
    "#     print(iteration)\n",
    "    label = word\n",
    "    for i in range(5):\n",
    "        imputed_word = gen_impurities(iteration,label)\n",
    "#         print(imputed_word)\n",
    "        if imputed_word not in seen:\n",
    "            seen[imputed_word] = 1\n",
    "            return imputed_word\n",
    "        seen[imputed_word] = seen[imputed_word]+1\n",
    "    return word\n",
    "            \n",
    "\n",
    "def gen_gibberish(min_l,max_l):\n",
    "    # initializing size of string\n",
    "    l = random.randint(min_l,max_l)\n",
    "    # using random.choices()\n",
    "    # generating random strings\n",
    "    res = ''.join(random.choices(string.ascii_uppercase +\n",
    "                                 string.punctuation +\n",
    "                                 string.digits, k=l))\n",
    "    while res not in seen:    \n",
    "        seen[res] = 1\n",
    "        return res\n",
    "    seen[res] = seen[res]+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e5b11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take top 100 US retailers arbitarily chosen based on their annual reported Sales\n",
    "\n",
    "def read_data():\n",
    "    df = pd.read_csv(\"stores.csv\")\n",
    "    del df['empty']\n",
    "    df = df.set_index('no')\n",
    "    df.head()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a9a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will muddle up the stores to simulate real world data capture where system might introduce impurites\n",
    "# the function will generate n number of instances for muddled data\n",
    "\n",
    "# n indicates number of obfuscated records\n",
    "# instance_count is the number of observation per class\n",
    "\n",
    "def get_obfuscated_stores(stores,n):\n",
    "    obfuscate_stores = stores * n\n",
    "    obfuscate_stores.sort()\n",
    "    df_obfuscated = pd.DataFrame({\"stores\": obfuscate_stores})\n",
    "    df_obfuscated['bad_names'] = df_obfuscated[\"stores\"].apply(lambda x: obfuscate(x) )\n",
    "    return df_obfuscated\n",
    "    \n",
    "def get_catch_gibberish(instance_count,min_l,max_l):\n",
    "    others = ['Other'] * instance_count\n",
    "    df_other = pd.DataFrame({'stores':others})\n",
    "    df_other['bad_names'] = df_other['stores'].apply(lambda x : gen_gibberish(min_l,max_l)) \n",
    "    return df_other\n",
    "\n",
    "# the training set will also have good captures where store name was interpreted correctly\n",
    "# for this purpose the we are creagin 10% bad captures\n",
    "\n",
    "def get_good_names(stores,n):\n",
    "    good_captures = stores*  n\n",
    "    df_good_captures = pd.DataFrame({'stores':good_captures})\n",
    "    df_good_captures['bad_names'] = df_good_captures[\"stores\"]\n",
    "    return df_good_captures\n",
    "\n",
    "def get_data(impute_n,instance,min_l,max_l): \n",
    "    n = impute_n\n",
    "    instance_count = instance\n",
    "    min_l = min_l\n",
    "    max_l = max_l\n",
    "\n",
    "    df_obfuscated = get_obfuscated_stores(stores,n)\n",
    "    df_other = get_catch_gibberish(instance_count,min_l,max_l)\n",
    "    df_good_captures = repeat_good_names(stores,n,instance_count)\n",
    "    df = pd.concat([df_obfuscated,df_good_captures,df_other])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c9de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will muddle up the stores to simulate real world data capture where system might introduce impurites\n",
    "# the function will generate n number of instances for jumbled data\n",
    "\n",
    "# n indicates number of obfuscated records\n",
    "# instance_count is the number of observation per class\n",
    "\n",
    "def get_obfuscated_stores(stores,n):\n",
    "    \"\"\"\n",
    "        This functions takes the stores list\n",
    "        Repeats it n times\n",
    "        calls teh obfuscate function to generate dirty names\n",
    "    \"\"\"\n",
    "    obfuscate_stores = stores * n\n",
    "    obfuscate_stores.sort()\n",
    "    df_obfuscated = pd.DataFrame({\"stores\": obfuscate_stores})\n",
    "    df_obfuscated['bad_names'] = df_obfuscated[\"stores\"].apply(lambda x: obfuscate(x) )\n",
    "    return df_obfuscated\n",
    "    \n",
    "def get_catch_gibberish(instance_count,min_l,max_l):\n",
    "    others = ['Other'] * instance_count\n",
    "    df_other = pd.DataFrame({'stores':others})\n",
    "    df_other['bad_names'] = df_other['stores'].apply(lambda x : gen_gibberish(min_l,max_l)) \n",
    "    return df_other\n",
    "\n",
    "# the training set will also have good captures where store name was interpreted correctly\n",
    "# for this purpose the we are creagin 10% bad captures\n",
    "\n",
    "def get_good_names(stores,n):\n",
    "    good_captures = stores*  n\n",
    "    df_good_captures = pd.DataFrame({'stores':good_captures})\n",
    "    df_good_captures['bad_names'] = df_good_captures[\"stores\"]\n",
    "    return df_good_captures\n",
    "\n",
    "def get_data(impute_n,instance,min_l,max_l): \n",
    "    n = impute_n\n",
    "    instance_count = instance\n",
    "    min_l = min_l\n",
    "    max_l = max_l\n",
    "\n",
    "    df_obfuscated = get_obfuscated_stores(stores,n)\n",
    "    df_other = get_catch_gibberish(instance_count,min_l,max_l)\n",
    "    df_good_captures = repeat_good_names(stores,n,instance_count)\n",
    "    df = pd.concat([df_obfuscated,df_good_captures,df_other])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea6e4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(split_ratio,impute_ratio,instance,min_l,max_l):\n",
    "    test_size = (instance*split_ratio)\n",
    "    train_size = (instance*(1-split_ratio))\n",
    "    print(f\"test_size = {test_size} & train_size = {train_size} & split = {split_ratio}\")\n",
    "    test_impute_n = int(test_size*impute_ratio)\n",
    "    test_good_n = int(test_size - test_impute_n)\n",
    "    print(f\"test impute = {test_impute_n} & instances = {test_good_n}\")\n",
    "    df = read_data()\n",
    "    stores = list(df[\"store\"])\n",
    "    # get imputed\n",
    "    df_obfuscated = get_obfuscated_stores(stores,test_impute_n)\n",
    "    # get good names\n",
    "    df_good_names = get_good_names(stores, test_good_n)\n",
    "    # get others \n",
    "    df_other = get_catch_gibberish(test_impute_n+test_good_n,min_l,max_l)\n",
    "    df_test = pd.concat([df_obfuscated,df_good_names,df_other])\n",
    "    \n",
    "    del df_obfuscated,df_good_names,df_other\n",
    "    train_impute_n = int(train_size*impute_ratio)\n",
    "    train_good_n = int(train_size - train_impute_n)\n",
    "    # get imputed\n",
    "    df_obfuscated = get_obfuscated_stores(stores,train_impute_n)\n",
    "    # get good names\n",
    "    df_good_names = get_good_names(stores, train_good_n)\n",
    "    # get others \n",
    "    df_other = get_catch_gibberish(train_impute_n+train_good_n,min_l,max_l)\n",
    "    \n",
    "    df_train = pd.concat([df_obfuscated,df_good_names,df_other])\n",
    "    \n",
    "    del df_obfuscated,df_good_names,df_other\n",
    "    return df_train, df_test\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1de37007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_size = 2000.0 & train_size = 8000.0 & split = 0.2\n",
      "test impute = 1000 & instances = 1000\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = get_train_test_data(0.2,0.5,10000,5,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9daf56a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df_train.groupby('bad_names').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d650b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.reset_index(inplace=True)\n",
    "t = t.rename(columns = {'index':'bad_names'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "53b4074d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad_names</th>\n",
       "      <th>stores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86087</th>\n",
       "      <td>7-Eleven</td>\n",
       "      <td>4943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117765</th>\n",
       "      <td>AT&amp;T Wireless</td>\n",
       "      <td>4643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120951</th>\n",
       "      <td>AVB Brandsource</td>\n",
       "      <td>4424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123879</th>\n",
       "      <td>Academy Sports</td>\n",
       "      <td>4505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125529</th>\n",
       "      <td>Ace Hardware</td>\n",
       "      <td>4638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323716</th>\n",
       "      <td>Wayfair</td>\n",
       "      <td>5346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326851</th>\n",
       "      <td>Wegmans Food Market</td>\n",
       "      <td>4311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328183</th>\n",
       "      <td>Weis Markets</td>\n",
       "      <td>4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331787</th>\n",
       "      <td>Williams-Sonoma</td>\n",
       "      <td>4389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333173</th>\n",
       "      <td>WinCo Foods</td>\n",
       "      <td>4725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  bad_names  stores\n",
       "86087              7-Eleven    4943\n",
       "117765        AT&T Wireless    4643\n",
       "120951      AVB Brandsource    4424\n",
       "123879       Academy Sports    4505\n",
       "125529         Ace Hardware    4638\n",
       "...                     ...     ...\n",
       "323716              Wayfair    5346\n",
       "326851  Wegmans Food Market    4311\n",
       "328183         Weis Markets    4600\n",
       "331787      Williams-Sonoma    4389\n",
       "333173          WinCo Foods    4725\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[t['stores'] >1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eb513ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len test stores = 101\n",
      "len train stores = 101\n"
     ]
    }
   ],
   "source": [
    "print(f\"len test stores = {len(set(df_test['stores']))}\")\n",
    "print(f\"len train stores = {len(set(df_train['stores']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "304398bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "def get_train_test_split(df_train,df_test,one_hot_encode_labels=False):\n",
    "    X_train = df_train[\"bad_names\"].values\n",
    "    y_train = df_train[\"stores\"].values\n",
    "    X_test  = df_test[\"bad_names\"].values\n",
    "    y_test  = df_test[\"stores\"].values\n",
    "    \n",
    "    if one_hot_encode_labels:\n",
    "        df_labels = pd.concat([df_train['stores'],df_test['stores']])\n",
    "        print(f\"Labels = {df_labels.shape})\")\n",
    "        lables = pd.get_dummies(df_labels)\n",
    "        lookup = list(lables.columns)\n",
    "        print(len(lookup))\n",
    "        del df_labels\n",
    "        y_test_labels = pd.get_dummies(y_test)\n",
    "        y_test_encoded = y_test_labels.astype('float32').values\n",
    "        y_train_labels = pd.get_dummies(y_train)\n",
    "        y_train_encoded = y_train_labels.astype('float32').values\n",
    "        return X_train,y_train_encoded,X_test,y_test_encoded, lookup\n",
    "    else:\n",
    "        return X_train, y_train, X_test, y_test, None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78fbc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, y_train, X_test, y_test, lable_lookup = get_train_test_split(df_train,\n",
    "                                                                      df_test,\n",
    "                                                                      one_hot_encode_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd812a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train - (808000,)\n",
      "Shape of y train - (808000,)\n",
      "shape of X test - (202000,)\n",
      "shape of y test (202000,)\n",
      "Test labels y train - 101\n",
      "Test labels X train - 335118\n",
      "Test labels y test - 101\n",
      "Test labels X test - 94379\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of X train - {X_train.shape}\")\n",
    "print(f\"Shape of y train - {y_train.shape}\")\n",
    "print(f\"shape of X test - {X_test.shape}\")\n",
    "print(f\"shape of y test {y_test.shape}\")\n",
    "print(f\"Test labels y train - {len(set(y_train))}\")\n",
    "print(f\"Test labels X train - {len(set(X_train))}\")\n",
    "print(f\"Test labels y test - {len(set(y_test))}\")\n",
    "print(f\"Test labels X test - {len(set(X_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9e10555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PetSmart', 'Academy Sports', 'Sprouts Farmers Market', 'Bed Bath & Beyond', 'Walmart', 'Michaels Stores', 'J.C. Penney Company', 'Signet Jewelers', 'The Kroger Co.', 'CVS Health Corporation', 'Urban Outfitters', 'Harbor Freight Tools', 'My Demoulas', 'Advance Auto', 'Hy Vee', 'Qurate Retail', 'TJX Companies', 'Hobby Lobby Stores', 'Weis Markets', 'Bath & Body Works', 'Defense Commissary Agency', 'RH', \"BJ's Wholesale Club\", 'Bass Pro', 'Giant Eagle', 'Albertsons Companies', 'Golub', 'Williams-Sonoma', \"Dillard's\", 'Aldi', 'Wakefern / ShopRite', \"Dick's Sporting Goods\", 'AutoZone', \"Hudson's Bay\", 'Office Depot', 'Ikea North America Services', 'Barnes & Noble', 'Wayfair', 'Target', 'Good Neighbor Pharmacy', 'WinCo Foods', 'Publix Super Markets', 'Tapestry', 'Exxon Mobil Corporation', 'Meijer', 'Menards', 'AT&T Wireless', 'Wegmans Food Market', 'Ingles', 'Alimentation Couche-Tard', 'Petco', 'Ross Stores', 'Burlington', 'Other', \"Lowe's Companies\", 'American Eagle Outfitters', 'Total Wine & More', \"Victoria's Secret\", \"Casey's General Store\", 'Verizon Wireless', 'Chewy.com', 'Nordstrom', 'Tractor Supply Co.', 'Discount Tire', '7-Eleven', 'Staples', 'Save-A-Lot', 'Walgreens Boots Alliance', 'Sherwin-Williams', \"Macy's\", 'The Home Depot', 'H.E. Butt Grocery', 'True Value Co.', 'Amazon.com', 'Shell Oil Company', 'Gap', 'Royal Ahold Delhaize USA', 'Southeastern Grocers', 'Save Mart', 'Ulta Beauty', 'Sephora (LVMH)', 'Costco Wholesale', 'Army and Air Force Exchange Service', 'Camping World', 'Big Lots', 'Stater Bros Holdings', 'Lululemon', 'AVB Brandsource', 'Rite Aid', 'Piggly Wiggly', 'Dell Technologies', 'Smart & Final', 'O’Reilly Auto Parts', 'Best Buy', 'Health Mart Systems', \"Kohl's\", 'Ace Hardware', 'Apple Stores / iTunes', 'Dollar General', 'Foot Locker', 'Dollar Tree']\n"
     ]
    }
   ],
   "source": [
    "LABELS = list(set(y_train))\n",
    "print(LABELS)\n",
    "\n",
    "def labels_to_index(label,labels=LABELS):\n",
    "    return labels.index(label)\n",
    "\n",
    "def index_to_labels(idx,labels=LABELS):\n",
    "    return labels[idx]\n",
    "\n",
    "def index_to_labels_arr(arr,labels=LABELS):\n",
    "    return labels[np.argmax(arr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "886bc214",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create indexed classes\n",
    "y_train_tx = [labels_to_index(y) for y in y_train]\n",
    "y_test_tx = [labels_to_index(y) for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07c359bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " y train indexed class - 64\n",
      " y train label 7-Eleven\n",
      " y test indexed class - 7-Eleven\n",
      " y test indexed class - 64\n"
     ]
    }
   ],
   "source": [
    "print(f\" y train indexed class - {y_train_tx[0]}\")\n",
    "print(f\" y train label {y_train[0]}\")\n",
    "print(f\" y test indexed class - {y_test[0]}\")\n",
    "print(f\" y test indexed class - {y_test_tx[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "212c29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9240e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "680e4432",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100\n",
    "sequence_length = 50\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    split = 'character',\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1464aa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 08:36:43.167322: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "train_text = raw_train.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ef59846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 'o',\n",
       " 's',\n",
       " 'r',\n",
       " 't',\n",
       " 'l',\n",
       " 'i',\n",
       " 'n',\n",
       " 'c',\n",
       " 'm',\n",
       " 'h',\n",
       " 'd',\n",
       " 'b',\n",
       " 'p',\n",
       " 'g',\n",
       " 'u',\n",
       " 'y',\n",
       " 'w',\n",
       " 'f',\n",
       " 'v',\n",
       " 'k',\n",
       " 'j',\n",
       " 'x',\n",
       " '7',\n",
       " 'z',\n",
       " '3',\n",
       " '8',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '6',\n",
       " '9',\n",
       " '4',\n",
       " '5',\n",
       " 'q',\n",
       " '’']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fda4151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text)[0], label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43b9a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 --->   \n",
      " 30 --->  8\n",
      "Vocabulary size: 40\n"
     ]
    }
   ],
   "source": [
    "print(\"3 ---> \",vectorize_layer.get_vocabulary()[3])\n",
    "print(\" 30 ---> \",vectorize_layer.get_vocabulary()[30])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cd6ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(y_train_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ac964c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size - 646400 val size - 161600\n"
     ]
    }
   ],
   "source": [
    "train_idx = int(size*0.8)\n",
    "val_idx = int(size*0.2)\n",
    "print(f\"train size - {train_idx} val size - {val_idx}\")\n",
    "\n",
    "X_traning_ds = X_train[0:train_idx]\n",
    "y_train_ds = y_train_tx[0:train_idx]\n",
    "X_val_ds = X_train[train_idx:train_idx+val_idx]\n",
    "y_val_ds = y_train_tx[train_idx:train_idx+val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "010eb9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - (646400,) Val - (161600,)\n",
      "Training - 646400 Val - 161600\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training - {X_traning_ds.shape} Val - {X_val_ds.shape}\")\n",
    "print(f\"Training - {len(y_train_ds)} Val - {len(y_val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "19c37de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_train = tf.data.Dataset.from_tensor_slices((X_traning_ds, y_train_ds))\n",
    "raw_val = tf.data.Dataset.from_tensor_slices((X_val_ds, y_val_ds))\n",
    "raw_test = tf.data.Dataset.from_tensor_slices((X_test, y_test_tx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a3b3bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = raw_train.map(vectorize_text)\n",
    "val_ds = raw_val.map(vectorize_text)\n",
    "test_ds = raw_test.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dfbcedd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[27  2  9  2 23  2 30 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]], shape=(1, 50), dtype=int64)\n",
      "tf.Tensor(64, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch = next(iter(train_ds))\n",
    "print(text_batch)\n",
    "print(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "311bc8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "fin_train_ds = train_ds.shuffle(buffer_size=BUFFER_SIZE).batch(batch_size=BATCH_SIZE,drop_remainder=True)\n",
    "fin_val_ds = val_ds.batch(batch_size=BATCH_SIZE,drop_remainder=True)\n",
    "fin_test_ds = test_ds.batch(batch_size=BATCH_SIZE,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9cfdcaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[32 27  2 ...  0  0  0]\n",
      " [27  2  9 ...  0  0  0]\n",
      " [32 27  2 ...  0  0  0]\n",
      " ...\n",
      " [30 27  2 ...  0  0  0]\n",
      " [27  2  9 ...  0  0  0]\n",
      " [27  2  9 ...  0  0  0]], shape=(128, 50), dtype=int64)\n",
      "tf.Tensor(\n",
      "[64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n",
      " 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n",
      " 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n",
      " 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n",
      " 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n",
      " 64 64 64 64 64 64 64 64], shape=(128,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch = next(iter(fin_train_ds))\n",
    "print(text_batch)\n",
    "print(label_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d4369e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "\n",
    "def get_model(embedding_dim,max_features):\n",
    "    model = tf.keras.Sequential([\n",
    "          layers.Embedding(max_features + 1, embedding_dim),\n",
    "          layers.Bidirectional(layers.LSTM(128)),\n",
    "          layers.Dropout(0.2),\n",
    "          layers.Dense(128,activation='tanh', kernel_regularizer='l2'),\n",
    "          layers.Dropout(0.2),\n",
    "          layers.Dense(101, activation='softmax')])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bb6142db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, None, 16)          1616      \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirectio  (None, 256)              148480    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 101)               13029     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 196,021\n",
      "Trainable params: 196,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(embedding_dim,max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e0dada49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The loss function and the metrics need to be compliant to get good results\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              optimizer= 'adam',\n",
    "              metrics=tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1e01be4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5050/5050 [==============================] - 191s 38ms/step - loss: 1.7296 - sparse_categorical_accuracy: 0.7328 - val_loss: 0.7028 - val_sparse_categorical_accuracy: 0.9509\n",
      "Epoch 2/10\n",
      "5050/5050 [==============================] - 189s 37ms/step - loss: 0.5485 - sparse_categorical_accuracy: 0.9349 - val_loss: 0.6568 - val_sparse_categorical_accuracy: 0.9509\n",
      "Epoch 3/10\n",
      "5050/5050 [==============================] - 186s 37ms/step - loss: 0.3551 - sparse_categorical_accuracy: 0.9655 - val_loss: 0.6813 - val_sparse_categorical_accuracy: 0.9509\n",
      "Epoch 4/10\n",
      "5050/5050 [==============================] - 187s 37ms/step - loss: 0.2226 - sparse_categorical_accuracy: 0.9815 - val_loss: 0.7143 - val_sparse_categorical_accuracy: 0.9509\n",
      "Epoch 5/10\n",
      "5050/5050 [==============================] - 186s 37ms/step - loss: 0.1928 - sparse_categorical_accuracy: 0.9850 - val_loss: 0.7319 - val_sparse_categorical_accuracy: 0.9509\n",
      "Epoch 6/10\n",
      "5050/5050 [==============================] - 186s 37ms/step - loss: 0.1245 - sparse_categorical_accuracy: 0.9918 - val_loss: 0.7570 - val_sparse_categorical_accuracy: 0.9509\n",
      "Epoch 7/10\n",
      "5050/5050 [==============================] - 185s 37ms/step - loss: 0.0999 - sparse_categorical_accuracy: 0.9943 - val_loss: 0.7702 - val_sparse_categorical_accuracy: 0.9509\n",
      "Epoch 8/10\n",
      "5050/5050 [==============================] - 188s 37ms/step - loss: 0.0851 - sparse_categorical_accuracy: 0.9954 - val_loss: 0.7570 - val_sparse_categorical_accuracy: 0.9509\n",
      "Epoch 9/10\n",
      "5050/5050 [==============================] - 188s 37ms/step - loss: 0.0627 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.7614 - val_sparse_categorical_accuracy: 0.9509\n",
      "Epoch 10/10\n",
      "5050/5050 [==============================] - 185s 37ms/step - loss: 0.0812 - sparse_categorical_accuracy: 0.9960 - val_loss: 0.7620 - val_sparse_categorical_accuracy: 0.9509\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    fin_train_ds,\n",
    "    validation_data=fin_val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a2638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.10 (ml)",
   "language": "python",
   "name": "ml-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
